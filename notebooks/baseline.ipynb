{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yjneo\\anaconda3\\envs\\hecto\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"prithivMLmods/Deep-Fake-Detector-v2-Model\"\n",
    "TEST_DIR = Path(\"./test_data\")  # test 데이터 경로\n",
    "\n",
    "# Submission\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # output 폴더 없으면 생성\n",
    "\n",
    "OUT_CSV = OUTPUT_DIR / \"baseline_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "TARGET_SIZE = (224, 224)\n",
    "NUM_FRAMES = 10  # 비디오 샘플링 프레임 수\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"비디오 프레임을 균등하게 샘플링\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def get_full_frame_padded(pil_img: Image.Image, target_size=(224, 224)) -> Image.Image:\n",
    "    \"\"\"전체 이미지를 비율 유지하며 정사각형 패딩 처리\"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.BICUBIC)\n",
    "    new_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_img.paste(img, ((target_size[0] - img.size[0]) // 2,\n",
    "                        (target_size[1] - img.size[1]) // 2))\n",
    "    return new_img\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"이미지 또는 비디오에서 RGB 프레임 추출\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    # 이미지 파일\n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            return [np.array(img)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    # 비디오 파일\n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        imgs: List[Image.Image],\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.imgs = imgs\n",
    "        self.error = error\n",
    "\n",
    "def preprocess_one(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    파일 하나에 대한 전처리 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path: 처리할 파일 경로\n",
    "        num_frames: 비디오에서 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        PreprocessOutput 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "              \n",
    "        imgs: List[Image.Image] = []\n",
    "        \n",
    "        for rgb in frames:     \n",
    "            imgs.append(get_full_frame_padded(Image.fromarray(rgb), TARGET_SIZE))\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, imgs, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yjneo\\anaconda3\\envs\\hecto\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\yjneo\\.cache\\huggingface\\hub\\models--prithivMLmods--Deep-Fake-Detector-v2-Model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_ID)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"Model config: num_labels={model.config.num_labels}\")\n",
    "if hasattr(model.config, 'id2label'):\n",
    "    print(f\"id2label: {model.config.id2label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(pil_images: List[Image.Image]) -> List[float]:\n",
    "    if not pil_images:\n",
    "        return []\n",
    "\n",
    "    probs: List[float] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(DEVICE, non_blocking=True) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        batch_probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "\n",
    "# 전처리 및 추론\n",
    "for file_path in tqdm(files, desc=\"Processing\"):\n",
    "    out = preprocess_one(file_path)\n",
    "    \n",
    "    # 1. 에러 로깅\n",
    "    if out.error:\n",
    "        print(f\"[WARN] {out.filename}: {out.error}\")\n",
    "    \n",
    "    # 2. 정상 추론\n",
    "    elif out.imgs:\n",
    "        probs = infer_fake_probs(out.imgs)\n",
    "        results[out.filename] = float(np.mean(probs)) if probs else 0.0\n",
    "    \n",
    "    # 3. 둘 다 없으면 0.0 (real)\n",
    "    else:\n",
    "        results[out.filename] = 0.0\n",
    "\n",
    "print(f\"Inference completed. Processed: {len(results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "# CSV 저장\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"Saved submission to: {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
