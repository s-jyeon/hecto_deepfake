{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yjneo\\anaconda3\\envs\\hecto\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import sklearn\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ID = \"prithivMLmods/Deep-Fake-Detector-v2-Model\"\n",
    "MODEL_ID = \"buildborderless/CommunityForensics-DeepfakeDet-ViT\"\n",
    "TEST_DIR = Path(\"./test_data\")  # test 데이터 경로\n",
    "\n",
    "# Submission\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # output 폴더 없으면 생성\n",
    "\n",
    "SAFE_MODEL_ID = MODEL_ID.replace(\"/\", \"_\")\n",
    "OUT_CSV = OUTPUT_DIR / f\"{SAFE_MODEL_ID}_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "# TARGET_SIZE = (224, 224)\n",
    "TARGET_SIZE = (384, 384)\n",
    "NUM_FRAMES = 10  # 비디오 샘플링 프레임 수\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"비디오 프레임을 균등하게 샘플링\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def get_full_frame_padded(pil_img: Image.Image, target_size=(384, 384)) -> Image.Image:\n",
    "    \"\"\"전체 이미지를 비율 유지하며 정사각형 패딩 처리\"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.BICUBIC)\n",
    "    new_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_img.paste(img, ((target_size[0] - img.size[0]) // 2,\n",
    "                        (target_size[1] - img.size[1]) // 2))\n",
    "    return new_img\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"이미지 또는 비디오에서 RGB 프레임 추출\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    # 이미지 파일\n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            return [np.array(img)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    # 비디오 파일\n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        imgs: List[Image.Image],\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.imgs = imgs\n",
    "        self.error = error\n",
    "\n",
    "def preprocess_one(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    파일 하나에 대한 전처리 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path: 처리할 파일 경로\n",
    "        num_frames: 비디오에서 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        PreprocessOutput 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "              \n",
    "        imgs: List[Image.Image] = []\n",
    "        \n",
    "        for rgb in frames:     \n",
    "            imgs.append(get_full_frame_padded(Image.fromarray(rgb), TARGET_SIZE))\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, imgs, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded: buildborderless/CommunityForensics-DeepfakeDet-ViT\n",
      "Model config: num_labels=2\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model...\")\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_ID,size={\"height\": 384, \"width\": 384}, do_resize=True)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"Model config: num_labels={model.config.num_labels}\")\n",
    "if hasattr(model.config, 'id2label'):\n",
    "    print(f\"id2label: {model.config.id2label}\") #real:0,fake:1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(pil_images: List[Image.Image]) -> List[float]:\n",
    "    if not pil_images:\n",
    "        return []\n",
    "\n",
    "    probs: List[float] = []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        inputs = processor(images=pil_images, return_tensors=\"pt\",do_resize = False)  # get_full_frame_padded() 가 있으므로 resize 중복 방지\n",
    "        inputs = {k: v.to(DEVICE, non_blocking=True) for k, v in inputs.items()}\n",
    "        logits = model(**inputs).logits\n",
    "        batch_probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        probs.extend(batch_probs.cpu().tolist())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 정의\n",
    "- 학습시킬 dataset 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class DeepFakeImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for folder in self.root_dir.iterdir():\n",
    "            if not folder.is_dir():\n",
    "                continue\n",
    "            if folder.name not in LABEL_MAP:\n",
    "                continue\n",
    "\n",
    "            label = LABEL_MAP[folder.name]\n",
    "\n",
    "            for img_path in folder.rglob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                    self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"labels\": label\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yjneo\\anaconda3\\envs\\hecto\\lib\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 502.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\yjneo\\\\workspace\\\\hecto_deepfake\\\\notebooks\\\\deepfakeface_raw'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip 파일만 다운로드\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./deepfakeface_raw\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"OpenRL/DeepFakeFace\",\n",
    "    repo_type=\"dataset\",          # 매우 중요\n",
    "    allow_patterns=[\"*.zip\"],\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축해제\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "raw_dir = Path(\"./deepfakeface_raw\")\n",
    "out_dir = Path(\"./deepfakeface_extracted\")\n",
    "\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for zip_path in raw_dir.glob(\"*.zip\"):\n",
    "    target_dir = out_dir / zip_path.stem\n",
    "    target_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 90000, 0: 30000})\n"
     ]
    }
   ],
   "source": [
    "# 라벨링 잘 됐는지 확인\n",
    "LABEL_MAP = {\n",
    "     \"wiki\": 0, # real \n",
    "     \"inpainting\": 1, # fake \n",
    "     \"insight\": 1, # fake \n",
    "     \"text2img\": 1, # fake \n",
    "     }\n",
    "dataset = DeepFakeImageDataset(\"./deepfakeface_extracted\")\n",
    "\n",
    "from collections import Counter\n",
    "labels = [label for _, label in dataset.samples]\n",
    "print(Counter(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/val 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "indices = np.arange(len(dataset))\n",
    "labels = [label for _, label in dataset.samples]\n",
    "\n",
    "train_idx, val_idx = train_test_split(\n",
    "    indices,\n",
    "    test_size=0.1,\n",
    "    stratify=labels,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108000\n",
      "12000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일부 데이터만 추출\n",
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "ratio = 0.05   #일부만 사용\n",
    "num_samples = int(len(train_dataset) * ratio)\n",
    "\n",
    "indices = np.random.choice(\n",
    "    len(train_dataset),\n",
    "    size=num_samples,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "small_train_dataset = Subset(train_dataset, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "val_ratio = 0.01\n",
    "num_val = int(len(val_dataset) * val_ratio)\n",
    "\n",
    "val_indices = np.random.choice(\n",
    "    len(val_dataset),\n",
    "    size=num_val,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "small_val_dataset = Subset(val_dataset, val_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(small_train_dataset))\n",
    "print(len(small_val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train 준비 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn 학습과 추론 입력 구조를 동일하게 유지 \n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack(images),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    small_train_dataset,  # Train data 비율 조절하기\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    small_val_dataset,  # val data 비율 조절하기\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone 먼저 freeze\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 34 / 200\n"
     ]
    }
   ],
   "source": [
    "# unfreeze\n",
    "N = 2  # 마지막 N개 block unfreeze\n",
    "for layer in model.vit.encoder.layer[-N:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "# 학습 되는 layer 확인 (선택)\n",
    "trainable = sum(p.requires_grad for p in model.parameters())\n",
    "total = sum(1 for _ in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# backbone 전체 freeze 일 경우\n",
    "for param in model.vit.parameters():\n",
    "    param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oprimizer/Scheduler\n",
    "Learning_Rate = 1e-3\n",
    "DEVICE = \"cuda\"\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=Learning_Rate,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=len(train_loader) * num_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=2, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False  # stop = False\n",
    "\n",
    "        if score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True  # stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training one epoch\n",
    "def train_one_epoch(model, train_loader, optimizer, epoch, num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    batch_bar = tqdm(\n",
    "        enumerate(train_loader),\n",
    "        total=len(train_loader),\n",
    "        desc=f\"Epoch [{epoch+1}/{num_epochs}] Train\",\n",
    "        leave=False,\n",
    "        position=1\n",
    "    )\n",
    "\n",
    "    for step, (inputs, labels) in batch_bar:\n",
    "        inputs = inputs.cuda(non_blocking=True)\n",
    "        labels = labels.cuda(non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits, labels.float()\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        avg_loss = total_loss / (step + 1)\n",
    "\n",
    "        batch_bar.set_postfix({\n",
    "            \"batch_loss\": f\"{loss.item():.4f}\",\n",
    "            \"avg_loss\": f\"{avg_loss:.4f}\",\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "\n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probs = torch.softmax(outputs.logits, dim=1)[:, 1]  # fake probability\n",
    "        all_probs.extend(probs.cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "    val_loss = total_loss / len(loader)\n",
    "    val_auc = roc_auc_score(all_labels, all_probs)\n",
    "    return val_loss, val_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # wandb\n",
    "# num_epochs = 10\n",
    "\n",
    "# import wandb\n",
    "\n",
    "# run = wandb.init(\n",
    "#     entity=\"yjneon339-kyonggi-university\",   # 팀명 또는 계정명\n",
    "#     project=\"dacon_hecto_deepfake\",          # 프로젝트명\n",
    "#     config={\n",
    "#         \"learning_rate\": Learning_Rate,\n",
    "#         \"architecture\": MODEL_ID,\n",
    "#         \"dataset\": 'kaggle',\n",
    "#         \"epochs\": num_epochs,\n",
    "#         \"batch_size\": train_loader.batch_size\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "best_val_auc = 0.0\n",
    "early_stopper = EarlyStopping(patience=2)\n",
    "epoch_bar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    # 1. Train (batch 로그는 내부에서 처리됨)\n",
    "    train_loss = train_one_epoch(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        num_epochs=num_epochs\n",
    "    )\n",
    "\n",
    "    # 2. Validation\n",
    "    val_loss, val_auc = validate(model, val_loader)\n",
    "\n",
    "    # 3. Epoch 단위 로그\n",
    "    epoch_bar.set_postfix({\n",
    "        \"train_loss\": f\"{train_loss:.4f}\",\n",
    "        \"val_loss\": f\"{val_loss:.4f}\",\n",
    "        \"val_auc\": f\"{val_auc:.4f}\"\n",
    "    })\n",
    "\n",
    "    # 4. Best model 저장\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    # 5. Early stopping\n",
    "    if early_stopper.step(val_auc):\n",
    "        epoch_bar.write(\n",
    "            f\"Early stopping triggered at epoch {epoch+1} \"\n",
    "            f\"(best val_auc={early_stopper.best_score:.4f})\"\n",
    "        )\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Path is not a file: 'unfreeze_2blocks_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 1. 모델 아티팩트 생성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m artifact \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mArtifact(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munfreeze_2blocks_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43martifact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munfreeze_2blocks_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# 2. wandb에 로그\u001b[39;00m\n\u001b[0;32m      6\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog_artifact(artifact)\n",
      "File \u001b[1;32mc:\\Users\\yjneo\\anaconda3\\envs\\hecto\\lib\\site-packages\\wandb\\sdk\\artifacts\\_validators.py:271\u001b[0m, in \u001b[0;36mensure_not_finalized.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ArtifactFinalizedError(fullname\u001b[38;5;241m=\u001b[39mmethod_fullname, obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\yjneo\\anaconda3\\envs\\hecto\\lib\\site-packages\\wandb\\sdk\\artifacts\\artifact.py:1501\u001b[0m, in \u001b[0;36mArtifact.add_file\u001b[1;34m(self, local_path, name, is_tmp, skip_cache, policy, overwrite)\u001b[0m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Add a local file to the artifact.\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m \n\u001b[0;32m   1476\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;124;03m    ValueError: Policy must be \"mutable\" or \"immutable\"\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(local_path):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath is not a file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_path\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1503\u001b[0m name \u001b[38;5;241m=\u001b[39m LogicalPath(name \u001b[38;5;129;01mor\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(local_path))\n\u001b[0;32m   1504\u001b[0m digest \u001b[38;5;241m=\u001b[39m md5_file_b64(local_path)\n",
      "\u001b[1;31mValueError\u001b[0m: Path is not a file: 'unfreeze_2blocks_model.pt'"
     ]
    }
   ],
   "source": [
    "# 1. 모델 아티팩트 생성\n",
    "artifact = wandb.Artifact('unfreeze_2blocks_model', type='model')\n",
    "artifact.add_file(\"unfreeze_2blocks_model.pt\")\n",
    "\n",
    "# 2. wandb에 로그\n",
    "wandb.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-elevator-11</strong> at: <a href='https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake/runs/yj7k3gmb' target=\"_blank\">https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake/runs/yj7k3gmb</a><br> View project at: <a href='https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake' target=\"_blank\">https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20260105_222928-yj7k3gmb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yjneo\\workspace\\hecto_deepfake\\notebooks\\wandb\\run-20260105_223215-tolf4kpd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake/runs/tolf4kpd' target=\"_blank\">dashing-aardvark-12</a></strong> to <a href='https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake' target=\"_blank\">https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake/runs/tolf4kpd' target=\"_blank\">https://wandb.ai/yjneon339-kyonggi-university/dacon_hecto_deepfake/runs/tolf4kpd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact 'unfreeze_2blocks_model:v0', 137.36MB. 1 files...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 00:00:00.4 (366.3MB/s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yjneo\\workspace\\hecto_deepfake\\notebooks\\artifacts\\unfreeze_2blocks_model-v0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yjneo\\AppData\\Local\\Temp\\ipykernel_13552\\3974906615.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTForImageClassification(\n",
       "  (vit): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTAttention(\n",
       "            (attention): ViTSelfAttention(\n",
       "              (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=384, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 아티팩트 가져오기\n",
    "import wandb\n",
    "\n",
    "run = wandb.init(project=\"dacon_hecto_deepfake\", job_type=\"inference\")\n",
    "\n",
    "artifact = run.use_artifact(\n",
    "    \"yjneon339-kyonggi-university/dacon_hecto_deepfake/unfreeze_2blocks_model:v0\",\n",
    "    type=\"model\"\n",
    ")\n",
    "\n",
    "artifact_dir = artifact.download()\n",
    "print(artifact_dir)\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model = ViTForImageClassification.from_pretrained(MODEL_ID)\n",
    "state_dict = torch.load(\n",
    "    f\"{artifact_dir}/unfreeze_2blocks_model.pt\",\n",
    "    map_location=DEVICE\n",
    ")\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "model.to(DEVICE)\n",
    "model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DIR = Path(\"C:/Users/yjneo/Downloads/open/test_data\")  # test 데이터 경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length: 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [23:30<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference completed. Processed: 500 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "\n",
    "# 전처리 및 추론\n",
    "for file_path in tqdm(files, desc=\"Processing\"):\n",
    "    out = preprocess_one(file_path)\n",
    "    \n",
    "    # 1. 에러 로깅\n",
    "    if out.error:\n",
    "        print(f\"[WARN] {out.filename}: {out.error}\")\n",
    "    \n",
    "    # 2. 정상 추론\n",
    "    elif out.imgs:\n",
    "        probs = infer_fake_probs(out.imgs)\n",
    "        results[out.filename] = float(np.mean(probs)) if probs else 0.0\n",
    "    \n",
    "    # 3. 둘 다 없으면 0.0 (real)\n",
    "    else:\n",
    "        results[out.filename] = 0.0\n",
    "\n",
    "print(f\"Inference completed. Processed: {len(results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission to: output\\buildborderless_CommunityForensics-DeepfakeDet-ViT_submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('C:/Users/yjneo/workspace/hecto_deepfake/sample_submission.csv')\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "# CSV 저장\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"Saved submission to: {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hecto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
