{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import sklearn\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_ID = \"prithivMLmods/Deep-Fake-Detector-v2-Model\"\n",
    "MODEL_ID = \"buildborderless/CommunityForensics-DeepfakeDet-ViT\"\n",
    "TEST_DIR = Path(\"./test_data\")  # test 데이터 경로\n",
    "\n",
    "# Submission\n",
    "OUTPUT_DIR = Path(\"./output\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)  # output 폴더 없으면 생성\n",
    "\n",
    "SAFE_MODEL_ID = MODEL_ID.replace(\"/\", \"_\")\n",
    "OUT_CSV = OUTPUT_DIR / f\"{SAFE_MODEL_ID}_auxhead_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".jfif\"}\n",
    "VIDEO_EXTS = {\".mp4\", \".mov\"}\n",
    "\n",
    "# TARGET_SIZE = (224, 224)\n",
    "TARGET_SIZE = (384, 384)\n",
    "NUM_FRAMES = 10  # 비디오 샘플링 프레임 수\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_frame_indices(total_frames: int, num_frames: int) -> np.ndarray:\n",
    "    \"\"\"비디오 프레임을 균등하게 샘플링\"\"\"\n",
    "    if total_frames <= 0:\n",
    "        return np.array([], dtype=int)\n",
    "    if total_frames <= num_frames:\n",
    "        return np.arange(total_frames, dtype=int)\n",
    "    return np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
    "\n",
    "def get_full_frame_padded(pil_img: Image.Image, target_size=(384, 384)) -> Image.Image:\n",
    "    \"\"\"전체 이미지를 비율 유지하며 정사각형 패딩 처리\"\"\"\n",
    "    img = pil_img.convert(\"RGB\")\n",
    "    img.thumbnail(target_size, Image.BICUBIC)\n",
    "    new_img = Image.new(\"RGB\", target_size, (0, 0, 0))\n",
    "    new_img.paste(img, ((target_size[0] - img.size[0]) // 2,\n",
    "                        (target_size[1] - img.size[1]) // 2))\n",
    "    return new_img\n",
    "\n",
    "def read_rgb_frames(file_path: Path, num_frames: int = NUM_FRAMES) -> List[np.ndarray]:\n",
    "    \"\"\"이미지 또는 비디오에서 RGB 프레임 추출\"\"\"\n",
    "    ext = file_path.suffix.lower()\n",
    "    \n",
    "    # 이미지 파일\n",
    "    if ext in IMAGE_EXTS:\n",
    "        try:\n",
    "            img = Image.open(file_path).convert(\"RGB\")\n",
    "            return [np.array(img)]\n",
    "        except Exception:\n",
    "            return []\n",
    "    \n",
    "    # 비디오 파일\n",
    "    if ext in VIDEO_EXTS:\n",
    "        cap = cv2.VideoCapture(str(file_path))\n",
    "        total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        if total <= 0:\n",
    "            cap.release()\n",
    "            return []\n",
    "        \n",
    "        frame_indices = uniform_frame_indices(total, num_frames)\n",
    "        frames = []\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        cap.release()\n",
    "        return frames\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessOutput:\n",
    "    def __init__(\n",
    "        self,\n",
    "        filename: str,\n",
    "        imgs: List[Image.Image],\n",
    "        error: Optional[str] = None\n",
    "    ):\n",
    "        self.filename = filename\n",
    "        self.imgs = imgs\n",
    "        self.error = error\n",
    "\n",
    "def preprocess_one(file_path: Path, num_frames: int = NUM_FRAMES) -> PreprocessOutput:\n",
    "    \"\"\"\n",
    "    파일 하나에 대한 전처리 수행\n",
    "    \n",
    "    Args:\n",
    "        file_path: 처리할 파일 경로\n",
    "        num_frames: 비디오에서 추출할 프레임 수\n",
    "    \n",
    "    Returns:\n",
    "        PreprocessOutput 객체\n",
    "    \"\"\"\n",
    "    try:\n",
    "        frames = read_rgb_frames(file_path, num_frames=num_frames)\n",
    "              \n",
    "        imgs: List[Image.Image] = []\n",
    "        \n",
    "        for rgb in frames:     \n",
    "            imgs.append(get_full_frame_padded(Image.fromarray(rgb), TARGET_SIZE))\n",
    "        \n",
    "        return PreprocessOutput(file_path.name, imgs, None)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return PreprocessOutput(file_path.name, [], str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTForImageClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTWithAuxHead(nn.Module):\n",
    "    def __init__(self, model_id, device):\n",
    "        super().__init__()\n",
    "\n",
    "        # 기존 모델 그대로 로드\n",
    "        self.model = ViTForImageClassification.from_pretrained(model_id)\n",
    "        self.model.to(device)\n",
    "\n",
    "        hidden = self.model.config.hidden_size\n",
    "        num_labels = self.model.config.num_labels\n",
    "\n",
    "        \n",
    "        # auxiliary head\n",
    "        self.aux_classifier = nn.Linear(hidden, num_labels)\n",
    "\n",
    "        # backbone freeze\n",
    "        for p in self.model.vit.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, pixel_values, labels=None, aux_weight=0.3):\n",
    "        outputs = self.model.vit(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "        cls = outputs.last_hidden_state[:, 0]\n",
    "    \n",
    "        logits = self.model.classifier(cls)\n",
    "        aux_logits = self.aux_classifier(cls)\n",
    "    \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            ce = nn.CrossEntropyLoss()\n",
    "            main_loss = ce(logits, labels)\n",
    "            aux_loss = ce(aux_logits, labels)\n",
    "            loss = main_loss + aux_weight * aux_loss\n",
    "    \n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": logits,\n",
    "            \"aux_logits\": aux_logits\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with auxiliary head...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: buildborderless/CommunityForensics-DeepfakeDet-ViT\n",
      "num_labels: 2\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model with auxiliary head...\")\n",
    "model = ViTWithAuxHead(MODEL_ID, DEVICE).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    size={\"height\": 384, \"width\": 384},\n",
    "    do_resize=True\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {MODEL_ID}\")\n",
    "print(f\"num_labels: {model.model.config.num_labels}\")\n",
    "print(f\"id2label: {model.model.config.id2label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_logits(imgs):\n",
    "    logits_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img in imgs:\n",
    "            out = model(pixel_values=img)\n",
    "            logit = out[\"logits\"].item()   # binary logit (1D)\n",
    "            logits_list.append(logit)\n",
    "\n",
    "    return logits_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fake_probs(imgs, w_main=0.7, w_aux=0.3):\n",
    "    inputs = processor(images=imgs, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs[\"logits\"]\n",
    "        aux_logits = outputs[\"aux_logits\"]\n",
    "\n",
    "        # logit-level ensemble\n",
    "        final_logits = w_main * logits + w_aux * aux_logits\n",
    "\n",
    "    probs = torch.softmax(final_logits, dim=-1)[:, 1]\n",
    "    return probs.cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 정의\n",
    "- 학습시킬 dataset 처리\n",
    "- ds = load_dataset(\n",
    "    \"Hemgg/deep-fake-detection-dfd-entire-original-dataset\",\n",
    "    streaming = True\n",
    ")\n",
    "- ds2 = load_dataset(\"OpenRL/DeepFakeFace\",\n",
    "                   streaming = True)\n",
    "\n",
    "- ds3 = load_dataset(\"UniDataPro/deepfake-videos-dataset\",\n",
    "                   cache_dir=\"C:/Users/yjneo/workspace/hecto_deepfake/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링 정의\n",
    "LABEL_MAP = {\n",
    "    \"wiki\": 0,           # real\n",
    "    \"inpainting\": 1,     # fake\n",
    "    \"insight\": 1,        # fake\n",
    "    \"text2img\": 1,       # fake\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jpeg augmentation\n",
    "import io\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class RandomJPEGCompression:\n",
    "    def __init__(self, quality_range=(30, 100), p=0.5):\n",
    "        self.quality_range = quality_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "\n",
    "        quality = random.randint(*self.quality_range)\n",
    "        buffer = io.BytesIO()\n",
    "        img.save(buffer, format=\"JPEG\", quality=quality)\n",
    "        buffer.seek(0)\n",
    "        return Image.open(buffer).convert(\"RGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random gamma\n",
    "import random\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "class RandomGamma:\n",
    "    def __init__(self, gamma_range=(0.7, 1.5), p=0.5):\n",
    "        self.gamma_range = gamma_range\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() > self.p:\n",
    "            return img\n",
    "\n",
    "        gamma = random.uniform(*self.gamma_range)\n",
    "        return TF.adjust_gamma(img, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentation transform 정의\n",
    "from torchvision import transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.RandomCrop(384),\n",
    "\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.2,\n",
    "        contrast=0.2,\n",
    "        saturation=0.05,\n",
    "    ),\n",
    "\n",
    "    RandomGamma(gamma_range=(0.7, 1.5), p=0.4),\n",
    "    RandomJPEGCompression(quality_range=(30, 100), p=0.4),\n",
    "\n",
    "    transforms.RandomApply(\n",
    "        [transforms.GaussianBlur(kernel_size=3)],\n",
    "        p=0.2\n",
    "    ),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(448),\n",
    "    transforms.CenterCrop(384),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "class DeepFakeImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        for folder in self.root_dir.iterdir():\n",
    "            if not folder.is_dir():\n",
    "                continue\n",
    "            if folder.name not in LABEL_MAP:\n",
    "                continue\n",
    "\n",
    "            label = LABEL_MAP[folder.name]\n",
    "\n",
    "            for img_path in folder.rglob(\"*\"):\n",
    "                if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                    self.samples.append((img_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # transform 적용 (Tensor 변환)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": image,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)  # label도 Tensor로\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n",
      "Fetching 4 files: 100%|██████████| 4/4 [00:09<00:00,  2.32s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/workspace/hecto_deepfake/notebooks/deepfakeface_raw'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip 파일로 다운로드\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "local_dir = \"./deepfakeface_raw\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"OpenRL/DeepFakeFace\",\n",
    "    repo_type=\"dataset\",          # ← 이 줄이 없어서 404가 난 것\n",
    "    allow_patterns=[\"*.zip\"],\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축해제\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "raw_dir = Path(\"./deepfakeface_raw\")\n",
    "out_dir = Path(\"./deepfakeface_extracted\")\n",
    "\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for zip_path in raw_dir.glob(\"*.zip\"):\n",
    "    target_dir = out_dir / zip_path.stem\n",
    "    target_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 90000, 0: 30000})\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "# 라벨링 확인\n",
    "full_train_dataset = DeepFakeImageDataset(\n",
    "    \"./deepfakeface_extracted\",\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "full_val_dataset = DeepFakeImageDataset(    # no augmentation\n",
    "    \"./deepfakeface_extracted\",\n",
    "    transform=val_transform\n",
    ")\n",
    "\n",
    "from collections import Counter\n",
    "labels = [label for _, label in full_train_dataset.samples]\n",
    "pos_ratio = sum(labels) / len(labels)\n",
    "print(Counter(labels))\n",
    "print(pos_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# real/fake 가중치\n",
    "pos_weight = (1 - pos_ratio) / pos_ratio\n",
    "pos_weight = torch.tensor(pos_weight).to(DEVICE)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test split\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(full_train_dataset))\n",
    "val_size = len(full_train_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_train_dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# 일부만 사용 ()\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_subset_size = min(500, len(train_dataset))\n",
    "val_subset_size = min(100, len(val_dataset))\n",
    "\n",
    "small_train_dataset = Subset(train_dataset, range(train_subset_size))\n",
    "small_val_dataset = Subset(val_dataset, range(val_subset_size))\n",
    "print(len(small_train_dataset))\n",
    "print(len(small_val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn 학습과 추론 입력 구조를 동일하게 유지 \n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack(images),\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=4,        # 먼저 0으로 시작\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model 구성\n",
    "- backbone을 freeze 하고 classifier만 학습시킨다.\n",
    "- optimizer, scheduler, earlystopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone 먼저 freeze\n",
    "# for param in model.vit.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unfreeze\n",
    "# N = 2  # 마지막 N개 block unfreeze\n",
    "# for layer in model.vit.encoder.layer[-N:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 4 / 202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 학습 되는 layer 확인 (선택)\n",
    "trainable = sum(p.requires_grad for p in model.parameters())\n",
    "total = sum(1 for _ in model.parameters())\n",
    "print(f\"Trainable params: {trainable} / {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oprimizer/Scheduler\n",
    "Learning_Rate = 1e-4\n",
    "DEVICE = \"cuda\"\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=Learning_Rate,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=len(train_loader) * num_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, min_delta=0.0001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, score):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            return False  # stop = False\n",
    "\n",
    "        if score < self.best_score + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True  # stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop  \n",
    "from torch.cuda.amp import autocast\n",
    "pos_weight = torch.tensor(pos_ratio, device=DEVICE)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "def train_one_epoch(model, loader, aux_weight):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(\n",
    "                pixel_values=batch[\"pixel_values\"],\n",
    "                labels=batch[\"labels\"],\n",
    "                aux_weight=aux_weight\n",
    "            )\n",
    "            loss = outputs[\"loss\"]\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_cpu(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary weight 제어\n",
    "def get_aux_weight(epoch):\n",
    "    if epoch < 10:\n",
    "        return 0.3\n",
    "    elif epoch < 20:\n",
    "        return 0.2\n",
    "    else:\n",
    "        return 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        probs = torch.softmax(outputs.logits, dim=1)[:, 1]  # fake probability\n",
    "        all_probs.extend(probs.cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "\n",
    "    val_loss = total_loss / len(loader)\n",
    "    val_auc = roc_auc_score(all_labels, all_probs)\n",
    "    return val_loss, val_auc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # wandb\n",
    "# num_epochs = 1\n",
    "# Learning_Rate = 1e-4 \n",
    "# import wandb\n",
    "\n",
    "# run = wandb.init(\n",
    "#     entity=\"yjneon339-kyonggi-university\",   # 팀명 또는 계정명\n",
    "#     project=\"dacon_hecto_deepfake\",          # 프로젝트명\n",
    "#     config={\n",
    "#         \"learning_rate\": Learning_Rate,\n",
    "#         \"architecture\": MODEL_ID,\n",
    "#         \"dataset\": 'hf_openrl',\n",
    "#         \"epochs\": num_epochs,\n",
    "#         \"batch_size\": train_loader.batch_size\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_probs, all_labels = [], []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=batch[\"pixel_values\"],\n",
    "            labels=batch[\"labels\"]  # loss 계산용 (aux_weight 없이)\n",
    "        )\n",
    "\n",
    "        logits = outputs[\"logits\"]          # [B, 2]\n",
    "        loss = outputs[\"loss\"]              # CE 기반 loss\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]  # fake class prob\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_probs.append(probs.cpu())\n",
    "        all_labels.append(batch[\"labels\"].cpu())\n",
    "\n",
    "    val_auc = roc_auc_score(\n",
    "        torch.cat(all_labels).numpy(),\n",
    "        torch.cat(all_probs).numpy()\n",
    "    )\n",
    "\n",
    "    return total_loss / len(loader), val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/50 [00:00<?, ?it/s]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:   2%|▏         | 1/50 [12:02<9:49:51, 722.27s/it, train_loss=0.7243, val_loss=0.7188, val_auc=0.6065]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:   4%|▍         | 2/50 [24:06<9:38:34, 723.23s/it, train_loss=0.7189, val_loss=0.7163, val_auc=0.6097]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:   6%|▌         | 3/50 [36:09<9:26:33, 723.27s/it, train_loss=0.7174, val_loss=0.7157, val_auc=0.6065]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:   8%|▊         | 4/50 [48:12<9:14:22, 723.09s/it, train_loss=0.7168, val_loss=0.7154, val_auc=0.6083]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:  10%|█         | 5/50 [1:00:15<9:02:24, 723.21s/it, train_loss=0.7163, val_loss=0.7144, val_auc=0.6155]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:  12%|█▏        | 6/50 [1:12:18<8:50:21, 723.23s/it, train_loss=0.7158, val_loss=0.7149, val_auc=0.6103]/workspace/venv/lib/python3.11/site-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "Training:  14%|█▍        | 7/50 [1:25:08<8:42:58, 729.74s/it, train_loss=0.7148, val_loss=0.7144, val_auc=0.6145]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m epoch_bar:\n\u001b[32m     12\u001b[39m     aux_weight = get_aux_weight(epoch)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maux_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     val_loss, val_auc = validate(model, val_loader)\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# tqdm에 실시간 표시\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, aux_weight)\u001b[39m\n\u001b[32m     19\u001b[39m     loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     21\u001b[39m scaler.scale(loss).backward()\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m scaler.update()\n\u001b[32m     25\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:416\u001b[39m, in \u001b[36mGradScaler.step\u001b[39m\u001b[34m(self, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28mself\u001b[39m.unscale_(optimizer)\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m]) > \u001b[32m0\u001b[39m\n\u001b[32m    414\u001b[39m ), \u001b[33m\"\u001b[39m\u001b[33mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m retval = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    418\u001b[39m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mstage\u001b[39m\u001b[33m\"\u001b[39m] = OptState.STEPPED\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[39m, in \u001b[36mGradScaler._maybe_opt_step\u001b[39m\u001b[34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, *args, **kwargs):\n\u001b[32m    313\u001b[39m     retval = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(v.item() \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    315\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/venv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:314\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, *args, **kwargs):\n\u001b[32m    313\u001b[39m     retval = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[33m\"\u001b[39m\u001b[33mfound_inf_per_device\u001b[39m\u001b[33m\"\u001b[39m].values()):\n\u001b[32m    315\u001b[39m         retval = optimizer.step(*args, **kwargs)\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "best_val_auc = 0.0\n",
    "early_stopper = EarlyStopping(patience=10, min_delta=0.0003)\n",
    "epoch_bar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n",
    "\n",
    "for epoch in epoch_bar:\n",
    "    aux_weight = get_aux_weight(epoch)\n",
    "    train_loss = train_one_epoch(model, train_loader, aux_weight)\n",
    "    val_loss, val_auc = validate(model, val_loader)\n",
    "\n",
    "    # tqdm에 실시간 표시\n",
    "    epoch_bar.set_postfix({\n",
    "        \"train_loss\": f\"{train_loss:.4f}\",\n",
    "        \"val_loss\": f\"{val_loss:.4f}\",\n",
    "        \"val_auc\": f\"{val_auc:.4f}\"\n",
    "    })\n",
    "\n",
    "    # # W&B 로깅\n",
    "    # wandb.log({\n",
    "    #     \"epoch\": epoch + 1,\n",
    "    #     \"train_loss\": train_loss,\n",
    "    #     \"val_loss\": val_loss,\n",
    "    #     \"val_auc\": val_auc\n",
    "    # })\n",
    "\n",
    "    # best model 저장\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "\n",
    "    # early stopping\n",
    "    if early_stopper.step(val_auc):\n",
    "        epoch_bar.write(\n",
    "            f\"Early stopping triggered at epoch {epoch+1} \"\n",
    "            f\"(best val_auc={early_stopper.best_score:.4f})\"\n",
    "        )\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델 가져오기\n",
    "print(\"Loading model...\")\n",
    "\n",
    "model = ViTWithAuxHead(MODEL_ID, DEVICE)\n",
    "ckpt = torch.load(\"best_model.pt\", map_location=DEVICE)\n",
    "\n",
    "model.load_state_dict(ckpt)\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "processor = ViTImageProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    size={\"height\": 384, \"width\": 384},\n",
    "    do_resize=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded from best_model.pt\")\n",
    "print(f\"num_labels: {model.model.config.num_labels}\")\n",
    "print(f\"id2label: {model.model.config.id2label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론하기\n",
    "files = sorted([p for p in TEST_DIR.iterdir() if p.is_file()])\n",
    "print(f\"Test data length: {len(files)}\")\n",
    "\n",
    "results: Dict[str, float] = {}\n",
    "\n",
    "# 전처리 및 추론\n",
    "for file_path in tqdm(files, desc=\"Processing\"):\n",
    "    out = preprocess_one(file_path)\n",
    "    \n",
    "    # 1. 에러 로깅\n",
    "    if out.error:\n",
    "        print(f\"[WARN] {out.filename}: {out.error}\")\n",
    "    \n",
    "    # 2. 정상 추론\n",
    "    elif out.imgs:\n",
    "        logits = infer_fake_logits(out.img)\n",
    "\n",
    "        if logits:\n",
    "            mean_logit = floar(np.mean(logits))\n",
    "            prob = 1 / (1 + np.exp(-mean_logit))\n",
    "            results[out.filename] = prob\n",
    "    \n",
    "    # 3. 둘 다 없으면 0.0 (real)\n",
    "    else:\n",
    "        results[out.filename] = 0.0\n",
    "\n",
    "print(f\"Inference completed. Processed: {len(results)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('C:/Users/yjneo/workspace/hecto_deepfake/sample_submission.csv')\n",
    "submission['prob'] = submission['filename'].map(results).fillna(0.0)\n",
    "\n",
    "# CSV 저장\n",
    "submission.to_csv(OUT_CSV, encoding='utf-8-sig', index=False)\n",
    "print(f\"Saved submission to: {OUT_CSV}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vit_aux)",
   "language": "python",
   "name": "vit_aux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
